---
title: 深度学习在图像处理中的应用
date: 2021.11.3
author: Aik
img: /source/images/xxx.jpg
top: true
hide: false
cover: true
coverImg: /images/1.jpg
password: 
toc: true
mathjax: true
summary: 包括图像分类、目标检测、图像分析分割等
categories: 深度学习
tags:
  - 图像处理
  - 深度学习
  - PyTorch
---

# 深度学习在图像处理中的应用

包括图像分类（AlexNet，VGGNet，GoogLeNet，ResNet）、目标检测以及图像分割等等，使用pytorch进行实现。

## 1.1 卷积神经网络基础

参考视频：[卷积神经网络](https://www.bilibili.com/video/BV1b7411T7DA?spm_id_from=333.999.0.0)

### 1.1.1 全连接层

全连接层，由许许多多的神经元构成而得来的。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201202751850.png" alt="image-20211201202751850" style="zoom: 33%;" />
将神经元按列进行排列，并且将列与列进行全连接，那么就能得到一个神经网络。
BP(back propagation)算法包括**信号的前向传播**和**误差的反向传播**两个过程。即计算误差输出时按从输入到输出的方向进行，而调整权值和阈值则从输出到输入的方向进行。

如下图的从左到右的正向传播过程中能够得到一个输出值，将这个输出值与我们期望的输出值进行对比，就能得到一个误差值

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201201450959.png" alt="image-20211201201450959" style="zoom: 33%;" />

通过计算每一个节点的偏导数，就能得到每个节点的误差梯度，然后我们将得到的损失值反向应用到损失梯度上，就达到了误差的反向传播过程。

下面通过一个实例来解释BP神经网络：

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201202024109.png" alt="image-20211201202024109" style="zoom:25%;" />

首先我们读入一张彩色的RGB图像，他的每个像素都含有三个值，就是三个RGB分量。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201202344028.png" alt="image-20211201202344028" style="zoom:25%;" />

然后将它进行灰度化处理，进行灰度化之后，如下图所示，可以看到他的每个像素都只有一个值了。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201203243414.png" alt="image-20211201203243414" style="zoom:25%;" />

最后进行二值化处理就得到了一个黑白图像，如下图

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201203503932.png" alt="image-20211201203503932" style="zoom:25%;" />

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201203555477.png" alt="image-20211201203555477" style="zoom:25%;" />

用五行三列的一个窗口在整个图像上进行滑动，每滑动一次就计算白色像素占整个像素的比例，在滑动到最右侧时要进行越界处理，最后能得到一个$5\times5$的矩阵。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201204013150.png" alt="image-20211201204013150" style="zoom: 25%;" />

把这个$5\times5$的矩阵按行进行展开，拼接成一个行向量。这样就可以把这个行向量当作输入到神经网络的一个**输入层**。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201204252390.png" alt="image-20211201204252390" style="zoom: 25%;" />

one-hot编码是常用的对标签进行编码的一种方式，可以保证最右侧一列每个输出的编码都不同， 这样就得到了一个**输出层**。

有了输入和期望的输出，就能对网络进行训练了。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201204734587.png" alt="image-20211201204734587" style="zoom:33%;" />

将输入层节点数设为25个，输出层设为10个，中间的隐层按情况进行设置，这样就能对神经网络进行完整的训练了。

### 1.1.2 卷积层-CNN中独特的网络结构

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201210505827.png" alt="image-20211201210505827" style="zoom: 33%;" />

卷积就是**一个滑动窗口**在特征图上**滑动并进行计算**。如上图，以一个3x3的卷积核为例，把卷积核覆盖到计算的特征层上面，也就是图中的橙色部分。将值分别进行相乘再相加，得到一个值，也就是粉色部分填写的值。这样每滑动一次能得到一个值，最后将$3\times3$的矩阵值填满也就得到了最终的卷积结果。

**卷积的目的是为了进行图像特征提取**

**卷积有两个特性**：

1. 拥有局部感知机制：因为滑动是局部的
2. 权值共享：因为在滑动计算过程中卷积核值是固定的

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201211158960.png" alt="image-20211201211158960" style="zoom: 33%;" />

**对于输入层而言，channel代表图像的通道数量，当输入图像为RGB彩色图像，则channel = 3；如果是灰度图像，则channel = 1。** 

**对于卷积核而言，卷积核的深度 = 卷积核channel数**

- 若当前卷积层的上一层为输入层，则channel数 = 输入图像channel数
- 无论输入图像的深度是多少，每经过一个卷积核都将变换为一个深度为1的特征图，一个卷积层之内可定义多个卷积核，当前卷积层上的各个卷积核会对上一层输入的每个feature map（特征图）分别执行卷积操作，即每个卷积核都会对应生成一个新的特征图feature map(不同的卷积核所提取的特征不同)。故而在下一层需要多少个特征图，本层就需要定义多少个卷积核，所以**卷积核的深度与传出的特征图的张数一致**。

普通的BP神经网络，假设如图中中间的隐藏层为1000个神经元，那么进行全连接时就会有921600000个参数生成。（**参数指连接层之间的权重参数**）

而卷积神经网络由于每个卷积核是固定的 ，一个卷积层所需要的参数就是25000个，这就体现了之前说的权值共享的特性。

前面所说的都是在一维的特征向量上进行卷积的，但实际过程中往往都是对多维的特征矩阵进行卷积操作。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201212610650.png" alt="image-20211201212610650" style="zoom: 25%;" />

如上图，如果输入一张彩色的RGB图像，那么就会有R G B三个分量，那么**卷积核的深度也要和输入特征矩阵的深度保持一致**，也要是三维。将卷积核的每个维度放在对应的维度上进行滑动卷积计算，最后再进行求和操作就得到了一个卷积层。

红色卷积核在第一个位置计算结果为0，绿色卷积核在第一个位置计算结果为0，蓝色卷积核在第一个位置计算结果为1，$0+0+1=1$，那么输出特征矩阵的第一个位置就是1，全部计算完就能得到一个输出特征矩阵。如果我们在用一组卷积核2进行计算，那么有会得到一个输出特征矩阵。将这些输出特征矩阵进行拼接那么就能得到完整的输出特征矩阵。 

- 卷积核的深度（channel）与输入特征层的深度（channel）要相同：如图，输入的是三维的，那么卷积核也要是三维的。
- 输出的特征矩阵深度（channel）与卷积核个数相同：因为如图，每个卷积核都会生成一个输出特征矩阵。

**如果加上偏移量bias那么该怎么计算呢**

只需要与对应的输出特征矩阵相加运算。例如上面卷积核1的偏移量为-1，那么结果就是$\begin{matrix}0&2\\0&0\end{matrix}$,也就是在原本的结果上加上一个偏置项（偏移量）。卷积核2的偏移量为1，那么结果就是$\begin{matrix}2&2\\3&2 \end{matrix}$	



**如果加上激活函数，那么又该如何进行计算呢？**

下面是两个常用的激活函数：

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201220705581.png" alt="image-20211201220705581" style="zoom: 33%;" />

首先要明白**为什么要使用激活函数**，是因为在我们的计算过程中这是一个线性的计算过程。引入非线性因素，使其具备解决非线性问题的能力，那么就需要通过一个非线性函数来达到这个目的。

现在常用的是Relu激活函数，因为，Sigmoid激活函数**达到饱和时梯度值变得非常小**，当值越来越大的时候，导数就基本**趋于0**了。所以当网络层数较深时容易出现**梯度消失**，在反向传播误差的过程中导数求起来也会很麻烦。

Relu函数的**导数就非常简单**，当$x$小于0的时候，导数就等于0，当$x>0$的时候，导数就是等于1。但它同时也有缺点，在反向传播过程中有一个**非常大的梯度经过**时，反向传播更新后可能会导致**权重分布中心小于0**，在进行正向传播时就可能会导致计算出的**结果是一个小于0的数**，如果小于0的话，那么经过Relu激活函数时就会**被过滤掉**，它的值就会始终等于0，这样它的反向传播就无法更新权重了。$x<0$的梯度都为0了，反向传播过程中就无法往前进行传播了，而且**权重进入失活**，并且失活后就无法再被激活的。所以在训练过程当中，不要一开始就使用一个特别大的学习率进行学习，这样很可能会导致很多神经元失活。



**如果卷积过程中出现越界时再怎么去处理？**

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201223322907.png" alt="image-20211201223322907" style="zoom: 33%;" />

如图出现越界问题时，一般通过padding的方式进行补0处理

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201223529902.png" alt="image-20211201223529902" style="zoom:33%;" />

在卷积操作过程中，矩阵经卷积操作后的尺寸由以下几个因素决定：

- 输入图片大小$W \times W$：图中为$4\times4$的大小
- Filter（过滤器）的大小$F \times F$ ：图中为$3\times3$的大小
- 步长S：图中为2
- padding的像素数P： 正常是左右上下各补1个P，一共2P，图中只在右边和下边补了一个P

经卷积后的矩阵尺寸大小计算公式为：
$$
N=(W-F+P)/S+1
 =(4-3+1)/2+1=2
$$
所以最终得到的特征矩阵的大小就为2.

### 1.1.3 池化层

**池化层的目的**是对特征图进行稀疏处理，减少数据运算量。

池化层和卷积层比较类似，但是与卷积层相比较会简单许多。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211202130739275.png" alt="image-20211202130739275" style="zoom: 33%;" />

如上图就是用一个$2\times2$的池化核进行**最大下采样**操作，将这个池化核放在左图的第一个位置上，然后找$\begin{matrix}1&1\\5&6 \end{matrix}$中的**最大值**，也就是6，于是右边输出的第一个位置就填6.

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211202131259074.png" alt="image-20211202131259074" style="zoom:33%;" />

如图是一个**平均下采样层**操作，与上面所说的最大下采样层最大的区别就是，拿第一个位置举例就是它是找$\begin{matrix}1&1\\5&6 \end{matrix}$这几个数的**平均值**，将平均值3.25作为输出。

**池化层**拥有以下几个**特点**：

- 没有训练参数

  卷积层的每个卷积核都会有自己的参数，像前面提到的RGB例子，每个卷积核都会有R G B三个参数。而池化层则没有参数，它只是在原本的特征层上求最大值或者平均数，也就不需要训练参数。

- 只改变特征矩阵的w和h，不改变深度（channel）

  就如上图所示是一个$4\times4$的特征层、深度为3，用一个$2\times2$、步距（stride）为2的池化核进行计算，那么最后得到的会是一个$2\times2$、深度为3的结果。

- 一般池化核大小（poolsize）和步距（stride）相同，并不是绝对的。

### 1.1.4 误差的计算

以下面一个三层的BP神经网络为例

<img src="C:\Users\xwy\AppData\Roaming\Typora\typora-user-images\image-20211204164844141.png" alt="image-20211204164844141" style="zoom:33%;" />

从左往右，第一层就是输入层，有两个节点：$x_1$和$x_2$。中间为隐层，有三个节点，其中以最上面的节点为例：

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211204165138340.png" alt="image-20211204165138340" style="zoom:33%;" />



最上面这个节点的输出就是$x_1$乘以它对应的权重$w_{11}$加上$x_2$乘以它对应的权重$w_{21}$，最后加上偏置$b_1$.通过激活函数$σ$就得到了隐层的第一个节点的输出。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211204165831311.png" alt="image-20211204165831311" style="zoom: 33%;" />

同理如上图，可以求出$y_1$和$y_2$的输出，最后一层一般用的是softmax激活函数。

因为输出的$y_1$、$y_2$不属于任意一个分布，而我们想要它**满足概率分布**，所以要进行softmax处理。

计算表达式如下：
$$
o_{i}=\frac{e^{y_{i}}}{\sum_{j} e^{y_{j}}}
$$
根据表达式计算出来的结果如下：
$$
o_{1}=\frac{e^{y_{1}}}{e^{y_{1}}+e^{y_{2}}} \quad o_{2}=\frac{e^{y_{2}}}{e^{y_{1}}+e^{y_{2}}}
$$
经过softmax处理后的所有输出节点**概率和为1，**即$o_1+o_2=1$。

进行**交叉熵损失（Cross Entropy Loss）**的计算时，针对不同问题，会有不同种的计算方法。

1. 针对多分类问题（softmax输出，所有输出概率和为1）
   $$
   H=-{\sum}_io^*_ilog(o_i)
   $$
   大部分用的都是这一个，它的输出只会归为某一个类别。比如要么是猫要么是狗，不可能同时为多个。
   
2. 针对二分类问题（sigmoid输出，每个输出节点之间互不相干）
   $$
   H=-\frac{1}{N} \sum_{i=1}^{N}\left[o_{i}^{*} \log o_{i}+\left(1-o_{i}^{*}\right) \log \left(1-o_{i}\right)\right]
   $$

3. 

其中$o^*_i$为真实标签值，$o_i$为预测值，默认$log$以$e$为底等于$ln$

本例中是使用softmax激活函数，代入计算公式可以得到损失值：
$$
Loss=-(o_1^*log(o_1)+o_2^*log(o2))
$$

### 1.1.5 误差的反向传播

